---
title: |
  | \vspace{5cm} \LARGE{ISyE 6404 CP.1 Part II: PH Regression}
  | \LARGE{Bootstrap \& Large p Small n Problems}
author:
- Yuan Gao, Kevin Lee, Akshay Govindaraj, 
- Yijun (Emma) Wan, Peter Williams, Ruixuan Zhang

date: "`r paste0('Date: ',Sys.Date())`"
output:
   pdf_document:
      fig_caption: true
      number_sections: false
mainfont: Times
fontsize: 11pt
header-includes:
  - \usepackage{titling}
  - \pretitle{\begin{flushleft}}
  - \posttitle{\end{flushleft}}
  - \preauthor{\begin{flushleft}}
  - \postauthor{\end{flushleft}}
  - \predate{\begin{flushleft}}
  - \postdate{\end{flushleft}}
---

\newpage
\tableofcontents

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages( library(survival))
suppressPackageStartupMessages( library(survminer))
suppressPackageStartupMessages( library(boot))
suppressPackageStartupMessages( library(ggplot2))
```

## Workload Distribution

Below is a description of tasks completed by each team member for this project:  

\begin{center}
\begin{tabular}{ |l|l| } 
\hline
Team Member & Task Description  \\
\hline
Yuan Gao & ...TBD \\ 
Kevin Lee & ...TBD  \\ 
Akshay Govindaraj & ...TBD  \\ 
Yijun (Emma) Wan & ...TBD  \\ 
Peter Williams & ...TBD  \\ 
Ruixuan Zhang & ...TBD  \\ 
\hline
\end{tabular}
\end{center}

\newpage

## 1. Bootstrap Method 

Use the bootstrap method to construct a 90% pointwise confidence interval (CI) of betacoefficient(s)
based on the PH-regression parameter-estimate(s). With the 10000
bootstrap samples you have the entire distribution of beta-estimate(s). Thus, you can use
the methods below to construct CIs.  
i) Calculate standard error (denoted as s.e.(beta-est)) of a beta-estimate from the 1000
bootstrapped beta-estimates. Use [ beta-est. +- 1.645*s.e.(beta-est.) ] as the largesample
CI.  

```{r ph_bootstrap, include = T, echo = F, fig.align='center',fig.height=3}

library(ggplot2)

library(boot)
library(survival)
data("kidney")


############Function for bootstrapping "Age"
bs_fun_age <- function(data, indices){
	bs_dat <- data[indices,]
  	res.cox <- coxph(Surv(time, status) ~ age + sex + disease + frail , data = bs_dat)
  	return(as.numeric(res.cox$coefficients["age"]))
}
############Sex
bs_fun_sex <- function(data, indices){
	bs_dat <- data[indices,]
  	res.cox <- coxph(Surv(time, status) ~ age + sex + disease + frail , data = bs_dat)
  	return(as.numeric(res.cox$coefficients["sex"]))
}
############Frail
bs_fun_frail <- function(data, indices){
	bs_dat <- data[indices,]
  	res.cox <- coxph(Surv(time, status) ~ age + sex + disease + frail , data = bs_dat)
  	return(as.numeric(res.cox$coefficients["frail"]))
}

###This function performs the bootstrap and displays the standard error to be used
bs_res_age <- suppressWarnings(boot(kidney, bs_fun_age, R=10000))
print(bs_res_age)
##Repeat for other variables
bs_res_sex <- suppressWarnings(boot(kidney, bs_fun_sex, R=10000))
bs_res_frail <- suppressWarnings(boot(kidney, bs_fun_frail, R=10000))
print(bs_res_sex)
print(bs_res_frail)
```
Use the above bootstrap standard errors for our confidence interval for the "age" effect estimate and the bootstrap mean given alongside the standard error estimates.
This leads us to the following confidence intervals through the above equation [ beta-est. +- 1.645*s.e.(beta-est.)
Age: ()
Sex: ()
DiseaseGN: ()
DiseaseAN: ()
DiseasePKD: ()
Frail: ()


ii) If the R-program provides 90% confidence interval from either normal or chi-square
approximation based on the large-sample theory, it would be worthwhile to compare
the results against the ones given above to examine what R-program is doing.  

The R program boot.ci() does indeed have a function that allows us to create a 90% confidence interval from normal approximation, though 
```{r bootstrap_norm, include = T, echo = F, fig.align='center',fig.height=3}
bs_ci <- suppressWarnings(boot.ci(bs_res_age, conf = 0.90, var.t0 = NULL, type = "norm"))
print(bs_ci)
```
This gives us a confidence interval that is quite similar to the confidence interval calculated from part (i), though the "normal approximation" interval is slightly narrow compared to the confidence interval in (i). This leads one to believe there is another factor involved in the R program's interval calculation, as the 1.645 used in (i) is based off of normal approximations using z. 

iii) If the distribution of the bootstrap samples of beta-estimate is not symmetric like
normal, use 10000*0.05 = 500th lower and upper percentiles as the second largesample
CI. This is the so-called Percentile-Bootstrap-CI.  
iv) Apply the Bias Correction (BC) Method to obtain BC-Bootstrap-CI. See Section 15.3
textbook for the details of the BC-method.  

The Bias Correction bootstrap method is essentially an extension of the above percentile bootstrap method. A prominent issue in utilizing the percentile bootstrap method for confidence intervals is the fact that the estimate from the original data is not used (only the quantiles of the bootstrap) and skewness is not accounted for. The BC method calculates the confidence interval bounds through a formula that involves two factors: the bias factor and the acceleration factor (which measures the rate of change of the standard error of the bootstrap sample). This is called the BCa confidence interval, and it is a simple option to implement in R: 

```{r bootstrap_bca, include = T, echo = F, fig.align='center',fig.height=3}
bs_ci_bca <- suppressWarnings(boot.ci(bs_res_age, conf = 0.90, var.t0 = NULL, type = "bca"))
print(bs_ci_bca)
```

As we can see with the BCa confidence interval, it is narrower compared to the calculation of part (i). It is slightly wider compared to the interval calculated using the "norm" type in R for normal approximation. 
We must also keep in mind that despite these differing numbers, they are all indeed very similar to each other regardless. Different sets of data would be necessary to distinguish between methodologies through the R output. 

v) A better way to construct the large-sample CI for situation in (iii) is to adjust the
percentiles in two tails for having 100 bootstrap samples in total. That is, use a trialand-error
to locate these two tail-percentiles (there are better methods to find them)
for a non-symmetric distribution. For example, one tail might have 700 bootstrap
samples, and the other tail has 300 bootstrap samples. The locations of these two tails
form the CI. Students can look into the so-called Highest-Probability-Density
Confidence Interval (HPD-CI) for a better version of this idea.


## 2. Large-p-Small-n Problems in PH-Regression (30%) 

1) Search the Internet using the key words from the title given in Task #2.  
2) Locate three publications with a section on real-life data analysis/example.  
3) Briefly summarize (in an half-page report for one publication)  
  i) what is the goal of the study,  
  ii) key ideas in solving the problem, and  
  iii) what do you learn from their real-life data study  
Pulication 1: 
Xu, R. (n.d.). Proportional Hazards Mixed Models: A Review with Applications to Twin Models.http://www.stat-d.si/mz/mz1.1/xu.pdf
Goal: used the twin pairs consisting of monozygotic and dizygotic twins to implement Proportional Hazards Mixed Models (PHMM) for mixed effects models with right-censored data. 
Key ideas:
1.	added random effects (Normal distribution) to traditional PH model which enable covariate by cluster interactions.
2.	took log, the model is written in the equivalent form of a linear transformation model.
3.	decomposed the variation in the transformed event time, into contributions from the fixed covariate effects, the random effects, and a fixed error distribution.
4.	used EM algorithm to find the estimators where the M-step implementation was similar to the standard Cox model, and the E-step involved Markov Chain Monte Carlo (MCMC) to approximate the conditional distribution of the random effects given data.
Study: standard cox model is just a beginning. Faced with the real world problem and data, there is more complex relationships. We need to capture mixed, or cluster specific, covariate effects. So, when we study the statistical methods we need to understanding the assumptions, mathematical procedure, results and applications. As a result, we can easily combine methods to analysis the real world problem. 


Publication 2: 
J Gui, H Li, Penalized Cox regression analysis in the high- dimensional and low-sample size settings, with application to microarray gene expression data, Bioinformatics, Volume 21, Issue 13, 1 July 2005, Pages 3001–3008, https://academic.oup.com/bioinformatics/article/21/13/3001/196819

Goal: Find a model with good prediction accuracy and parsimony property to predict the classified outcome of different types of cancer using gene expression profiles of the cancerous cells as predictors under restriction of high-dimensionality and low-sample-size.

Key ideas in solving the problem:

The article use statistical method mainly including Cox proportional hazard regression model and Lasso estimation to conduct a LARS-Cox procedure which is based on LARS algorithm.

1.	Collect data about micro array gene expression and categorical outcome.

2.	Use Cox PH regression model to build up log-partial-likelihood function with respect to coefficients $\beta$ of gene expression signatures.

3.	Use Lasso estimate for estimating $\beta$ subject to the summation of parameters $\beta$’s is less than a tuning parameter s that determine how many covariates with coefficients are zero.

4.	Minimize the Cross-validated partial likelihood to choose the best value of tuning parameter s.

5.	Minimize a specific expansion of PH likelihood function (constructed through one-term Taylor Series) using iterative method and find the estimate of $\beta$ with smallest variance.

6.	Select gene expression profiles using the LARS-Cox produce and compare the prediction results with other methods.
It turns out that the LARS-Cox procedure is very useful in identifying genes expressions, building a parsimonious predictive model and classifying patients into relevantly higher-risk and lower-risk groups.

Study:
Based on standard Cox proportional hazard regression model, researchers often use different statistical method combined with the standard model to optimize the procedure and outcome of regression, which is based on practice of real-life data analysis and adequate statistical knowledges. In this way, it becomes possible to optimize a technical problem such as high-dimensionality and small-sample-size in real life application.

Publication 3:
Ping Wang, Yan Li, Chandan K. Reddy, 2017. Machine Learning for Survival Analysis: A Survey. ACM
Comput. Surv. 1, 1, Article 1 (March 2017), 38 pages.
DOI: 0000001.0000001
http://dmkd.cs.vt.edu/papers/CSUR17.pdf

Goal: 
The primary goal of survival analysis is to predict the occurrence of specific events of interest at future time points. In this survey, they discussed several topics that are closely related to survival analysis and illustrate several successful applications in various real-world application domains. This paper provided a more thorough understanding of the recent advances in survival analysis and offer some guidelines on applying these approaches to solve new problems that arise in applications with censored data.

Key ideas: 
1. This article provided a comprehensive review of the conventional survival analysis methods and various machine learning methods for survival analysis, and described other related topics along with the evaluation metric. 

2. Researches introduced the basic notations and concepts in survival analysis, including the structure of survival data and the common functions used in survival analysis. 

3. Then, they introduced the well-studied statistical survival methods and the representative machine learning based survival methods. 

4. Furthermore, they discussed the related topics in survival analysis, including data transformation, early prediction and complex events. 

5. They also provided the implementation details of these survival methods and described the commonly used performance evaluation metrics for these models. 

Study: 
Due to the widespread availability of longitudinal data from various real-world domains combined with the recent developments in various machine learning methods, there is an increasing demand for understanding and improving methods for effectively handling survival data.

Traditionally, statistical approaches have been widely developed in the literature to overcome this censoring issue. In addition, many machine learning algorithms are adapted to effectively handle survival data and tackle other challenging problems that arise in real-world data.

Besides the traditional applications in healthcare and biomedicine, survival analysis was also successfully applied in various real-world problems, such as reliability, student retention and user behavior modeling. 

  
    
    
*Questions?*  
*Contact: ygao390, kylee20, ywan40, agovindaraj6, pwilliams60, rzhang438 | @gatech.edu*


